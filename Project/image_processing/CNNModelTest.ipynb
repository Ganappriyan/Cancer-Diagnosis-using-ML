{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from cnnmodel import CNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 512, 512, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 512, 512, 16)      448       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 512, 512, 16)      2320      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 170, 170, 16)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 170, 170, 32)      4640      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 170, 170, 32)      9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 56, 56, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 56, 56, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 56, 56, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 18, 18, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 18, 18, 32)        18464     \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 18, 18, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 6, 6, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1152)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               295168    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 428,501\n",
      "Trainable params: 428,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 500 images belonging to 5 classes.\n",
      "Found 100 images belonging to 5 classes.\n",
      "Epoch 1/20\n",
      "16/16 [==============================] - 9s 366ms/step - loss: 1.5875 - accuracy: 0.1960 - val_loss: 1.5326 - val_accuracy: 0.2000\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 5s 295ms/step - loss: 1.5292 - accuracy: 0.2000 - val_loss: 1.5978 - val_accuracy: 0.2000\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 5s 291ms/step - loss: 1.5666 - accuracy: 0.2000 - val_loss: 1.4390 - val_accuracy: 0.2000\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 5s 305ms/step - loss: 1.4993 - accuracy: 0.2000 - val_loss: 1.5951 - val_accuracy: 0.2000\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 5s 298ms/step - loss: 1.6094 - accuracy: 0.2000 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 5s 293ms/step - loss: 1.6094 - accuracy: 0.2000 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 5s 291ms/step - loss: 1.6094 - accuracy: 0.2000 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 5s 288ms/step - loss: 1.6094 - accuracy: 0.2000 - val_loss: 1.6094 - val_accuracy: 0.2000\n"
     ]
    }
   ],
   "source": [
    "# New Model\n",
    "cnnmodel = CNNModel(\"model\")\n",
    "cnnmodel.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saved Model\n",
    "# cnnmodel = CNNModel(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/test/0/0.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image \u001b[39m=\u001b[39m load_img(\u001b[39m\"\u001b[39;49m\u001b[39mdata/test/0/0.jpg\u001b[39;49m\u001b[39m\"\u001b[39;49m, target_size\u001b[39m=\u001b[39;49m(\u001b[39m256\u001b[39;49m, \u001b[39m256\u001b[39;49m))\n\u001b[1;32m      2\u001b[0m cnnmodel\u001b[39m.\u001b[39mpredict(image\u001b[39m=\u001b[39mimg_to_array(image)\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m3\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/mlp/lib/python3.8/site-packages/keras/utils/image_utils.py:393\u001b[0m, in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    391\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, pathlib\u001b[39m.\u001b[39mPath):\n\u001b[1;32m    392\u001b[0m     path \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(path\u001b[39m.\u001b[39mresolve())\n\u001b[0;32m--> 393\u001b[0m   \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(path, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    394\u001b[0m     img \u001b[39m=\u001b[39m pil_image\u001b[39m.\u001b[39mopen(io\u001b[39m.\u001b[39mBytesIO(f\u001b[39m.\u001b[39mread()))\n\u001b[1;32m    395\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/test/0/0.jpg'"
     ]
    }
   ],
   "source": [
    "image = load_img(\"data/test/0/0.jpg\", target_size=(256, 256))\n",
    "cnnmodel.predict(image=img_to_array(image).reshape(1, 256, 256, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
